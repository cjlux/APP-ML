\documentclass[10pt,serif,mathserif,compress,hyperref={colorlinks}]{beamer}
\mode<presentation>
\usepackage{pgf}
\usepackage{pgfpages}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{comment}
\usepackage{geometry}
\usepackage[most]{tcolorbox}
\tcbuselibrary{skins}
\usepackage{beamerthemesplit}
\usepackage{amsmath, amsfonts, epsfig, xspace}
\usepackage{pstricks,pst-node}
\usepackage{multimedia}
\usepackage{wasysym}
\usepackage{animate}

\usepackage{graphicx}% for including figures
\usepackage{tikz}
\usetikzlibrary{positioning,decorations.pathreplacing,arrows}

\setlength{\parindent}{0pt}

\input{colors}
\input{commands}

\usetheme{jlcKeynote}
\useoutertheme[subsection=false]{miniframes}
\setbeamercolor{background canvas}{bg=gray!50!white}
\setbeamercolor{structure}{bg=white, fg=gray}
\setbeamertemplate{itemize items}{\gray{$\CIRCLE$}}

\hypersetup{linkcolor=Blue}
\hypersetup{citecolor=DeepPink4}
\hypersetup{urlcolor=DarkBlue}
\hypersetup{anchorcolor=Magenta}

\title[\hspace*{.5\linewidth}\insertframenumber/\inserttotalframenumber]
      {\fontsize{20}{20}\selectfont{Apprentissage Par Projet [APP]\\[3mm]Comprendre et utiliser le\\ \em{Machine learning}}}
%\subtitle{novembre 2021}
      \author[{\tiny nov 2021 -- V1.0}\hspace*{4cm}]{{\fontsize{8}{8}\selectfont{Jean-Luc.Charles\,@\,ENSAM.EU}}\\[5mm]
        \includegraphics[height=1.5cm]{images/logo-am-couleur-72dpi.jpg}}
\institute{}
\date{{\tiny novembre 2021}}
\titlegraphic{\vspace*{-0.5cm}\includegraphics[height=2.cm]{images/robot.png}}

\logo{}
\tcbset{enhanced, boxrule=0.2pt, sharp corners, drop lifted shadow, colback=Chocolate!25!white,colframe=Chocolate!75!black, fonttitle=\large}

\renewcommand\ttdefault{lmtt}

\begin{document}

\frame[plain]{\titlepage}

\setbeamercolor{structure}{fg=gray!50!white}

\section{AI}

\subsection{Aspect historique}

%===============================================================================
\begin{frame}{L'aspect historique...}
  \hspace*{-5mm}\includegraphics[width=1.1\textwidth]{images/AI-ML-DL_Nvidia-1.png}
  \vspace*{-7mm}
  \center{\tiny (crédit : \href{https://developer.nvidia.com/deep-learning}{developer.nvidia.com/deep-learning})}
\end{frame}
%===============================================================================

\subsection{Intelligence Artificielle}

%===============================================================================
\begin{frame}{Intelligence Artificielle ?}

  \bfdarkchoco{Intelligence Artificielle}
  \footnote{{\tiny utilisé la première fois en 1956 par \href{https://en.wikipedia.org/wiki/John\_McCarthy\_\%28computer\_scientist\%29}{John McCarthy},
      chercheur à Stanford lors de la conférence de Dartmouth}} :
  reste un terme ambigu aux définitions multiples :

  \begin{itemize}

  \item <2-> {\em ''...the science of making computers do things that require intelligence when done by humans.}''
    {\tiny \href{http://www.alanturing.net/turing\_archive/pages/reference\%20articles/what\%20is\%20ai.html}{Alan Turing, 1940}}\\[3mm]    

  \item <2-> {\em ''the field of study that gives computers the ability to learn without being explicitly programmed.''}
        {\tiny  \href{http://infolab.stanford.edu/pub/voy/museum/samuel.html}{Arthur Samuel, 1960}}

  \item <2-> {\em ''A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P,
    if its performance at tasks in T, as measured by P, improves with experience E.''}
    {\tiny \href{https://www.cs.cmu.edu/~tom/}{Tom Mitchell, 1997}}

  \item <2-> Notion d'{\em agent intelligent} ou d'{\em agent rationnel}\\
    {\em ''...agent qui agit de manière à
    atteindre la meilleure solution ou, dans un environnement incertain, la meilleure solution prévisible.}''
    {\tiny  \hyperlink{refRusselNorvig}{Stuart Russel, Peter Norvig, ``Intelligence Artificielle'' 2015}}

  \end{itemize}

\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{Intelligences Artificielles ?}
  
  \visible<1->{\textbf{\em IA Forte}} ({\em Strong AI})
  \visible<2->{%
    \begin{itemize}
    \item Vise à concevoir des systèmes qui pensent exactement comment les humains.
    \item Peut contribuer à expliquer comment les humains pensent...
    \item On en est encore loin... veut-on vraimment aller jusque là ?
    \end{itemize}
    }

  \medskip
  
  \visible<1->{%
    \textbf{\em IA Faible} ({\em Weak AI})}
  \visible<3->{%
    \begin{itemize}
    \item Vise à concevoir des systèmes qui peuvent ``se comporter'' comme des humains.
    \item Ne nous dit rien sur la façon dont les humains pensent.
    \item On y est déjà... On l'utilise tous les jours ! \\
      reconnaissance faciale, vocale, anti-spam, traduction...
    \end{itemize}
    }
   
\end{frame}
%===============================================================================

\section{Machine Learning}

\subsection{Branches}

%===============================================================================
\begin{frame}{{\em Machine Learning} et IA}

  {\small Page extraite de \href{https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12}
  {medium.com/machine-learning-for-humans/...}}\\[2mm]
  
  \hspace*{-10mm}\includegraphics[width=1.2\textwidth]{images/AI-from_MachineLearningForHumans.png}
  \vspace*{-8mm}
  
\end{frame}
%===============================================================================

\subsection{technics}

%===============================================================================
\begin{frame}{Machine Learning et IA}


  Plusieurs approches permettent de concevoir des algorithmes de {\em Machine Learning} :
    \begin{itemize}
    \item <1-> Programmation Génétique ({\em Genetic programming})
    \item <1-> Inférence bayésienne ({\em Bayesian inference})
    \item <1-> Logique Floue ({\em Fuzzy logic})
    \item <1-> Réseaux de neurones ({\em Neural Networks})
    \item <1-> ...
    \end{itemize}    

    \bigskip
    \visible <2->{La suite traite uniquement des \bfdarkchoco{Réseau de neurones artificiels}.}
\end{frame}
%===============================================================================

\section{Neural Networks}

\subsection{Artificial neuron}

%===============================================================================
\begin{frame}{Neurone artificiel}
  
  \tikzset{%
  neuron/.style={
    circle,
    draw,
    minimum size=.7cm,
    font=\normalsize
  },
  squa/.style={
    draw,
    inner sep=2pt,
    font=\normalsize,
    join = by -latex
  },
  }

  
  \begin{tcolorbox}[title=Le modèle informatique du neurone artificiel]  

  \hspace*{-1.cm}\begin{tikzpicture}[x=1.5cm, y=.7cm, >=stealth]

  \node [label=above:\parbox{2cm}{\centering {\em input}\\[2mm]}] at (0, 1.5) (x1)  {$x_1$};
  \node [] at (0, 0.5) (x2) {$x_2$};
  \node [] at (0, -0) (vdots) {$\vdots$};
  \node [] at (0, -0.7) (xn) {$x_n$};
  \node [label=above:\parbox{2cm}{\centering {\em bias}}] at (1.5, 1.8) (bias) {$-1$};
  \node [squa, label=above:{\parbox{2cm}{\centering {\em activation\\function}\\[5mm]}}] at (3.8, 0.15) (F) {$f$};
  \node [label=above:\parbox{2cm}{\centering {\em output}\\[5mm]}] at ((5.5, 0.15) (y) {$y = f(\sum_i{w_{i}\,x_i} - b)$};
  
  \node [neuron/.try] (output) at (1.5,0.15) {{$\displaystyle\Sigma$}};
  
  \draw [o-latex] (x1) -- (output);
  \draw [o-latex] (x2) -- (output);
  \draw [o-latex] (xn) -- (output);
  \draw [o-latex] (bias) -- (output);
  \draw [->] (output) -- (F);
  \draw [->] (F) -- (y);

  \node [] at (1.7,1) () {$b$} ;
  \node [] at (.7,1.1) () {$w_1$} ;
  \node [] at (.7,.5) () {$w_2$} ;
  \node [] at (.7, -0.6) () {$w_n$} ;
  \node [] at (2.6, -.15) () {\small $\sum_i{w_{i}\,x_i} - b$};
\end{tikzpicture}

  \end{tcolorbox}
  
  \visible<2->{Un \bfdarkchoco{neurone artificiel}:
    \begin{itemize}
    \item <3-> reçoit les données d'entrée $(x_{i})_{i=1..n}$ affectées des  \textbf{poids} $(w_i)_{i=1..n}$ ({\em weights}) 
    \item <4-> calcule la \textbf{somme pondérée} de ses entrées moins le biais $\sum_i{w_{i}\,x_i} - b$
    \item <5-> produit en sortie une \textbf{activation} $f(\sum_i{w_{i}\,x_i} - b)$, calculée avec une fonction \textbf{non-linéaire} $f$.
    \end{itemize}
  }

\end{frame}
%===============================================================================

\subsection{Activation functions}

%===============================================================================
\begin{frame}{Neurone artificiel}

  La fonction d'activation d'un neurone :
  \begin{itemize}
  \item indroduit un comportement non-linéaire,
  \item fixe la plage de sortie de l'activation,
    par exemple $[-1, 1]$, $[0, 1]$ ou encore $[0, \infty[$.
  \end{itemize}    

  \begin{tcolorbox}[title={\small Fonctions d'activation souvent utilisées en ML}]
    \includegraphics[width=\textwidth]{images/activ_functions-2.png}
  \end{tcolorbox}

  \begin{itemize}
  \item Le biais $b$ fixe le seuil d'activation du neurone.
  \end{itemize}    
   
\end{frame}
%===============================================================================

\subsection{Neural Networks}

%===============================================================================
\begin{frame}{Réseaux de neurones étudiés}

  \begin{itemize}
  \item Les réseaux de neurones sont des assemblages plus ou moins complexes de neurones artificiels\\
  \includegraphics[width=.2\textwidth]{images/RN.png}
  \item Deux architectures sont étudiées dans l'APP2 \footnote{\tiny{Apprentissage Par Problème}}
  pour la reconnaissance des chiffres  manuscrits de la banque \href{http://yann.lecun.com/exdb/mnist/}{MNIST} :

  \begin{itemize}
  \item Un \textbf{réseau dense}, simple, généraliste, pouvant procurer un score voisin de 95\% de réussite.
  \item Un \textbf{réseau convolutif} plus complexe,  spécialisé dans la reconnaissance des images,
    pouvant atteindre un score de 99\%.
  \end{itemize}       
  \end{itemize}       

  
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{Données utilisées pour entraîner les réseaux}

  \begin{itemize}
  \item \href{http://yann.lecun.com/exdb/mnist/}{MNIST} : banque de 70000 \textbf{images labellisées}\\[3mm]
    \includegraphics[width=0.7\textwidth]{images/MNIST_digits_sample.png}
  \item Images en ton de gris de 28 $\times$ 28 pixels
  \item 60000 images d'entraînement et 10000 images de test.
  \end{itemize}       
  
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}
  
  \hspace*{-5mm}\includegraphics[width=1.1\textwidth]{images/archiReseau-2.png}
  \vspace*{-2mm}
  {\footnotesize
  \begin{itemize}
  \item Matrice 28$\,\times\,$28 $\leadsto$ vecteur normalisé de 784 composantes \texttt{float} dans [0;1].
  \item Une couche d'entrée ({\em Input layer}) de 784 valeurs.
  \item Une couche cachée ({\em Hidden layer}) de 784 neurone, fonction d'activation {\em relu}.
  \item Une couche de sortie ({\em Output layer}) de 10 neurones (1 pour chaque chiffre à reconnaître), fonction d'activation {\em softmax}.
  \end{itemize}
  }

  
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}

  \begin{tcolorbox}[title=Fonction d'activation {\em softmax}]  

    \begin{minipage}{.4\textwidth}
      \hspace*{-5mm}\includegraphics[width=1.2\textwidth]{images/softmax-2.png}
    \end{minipage}
    \begin{minipage}{.6\textwidth}
      {\small
        \begin{itemize}
  \item Utilisée pour la \textbf{classification}.
  \item L'activation du neurone $k$ est $Y_k = e^{y_k}/\sum_i{e^{y_i}}$ 
    avec $y_k = \sum_i \omega_i x_i - b$ calculé par le neurone $k$.
  \item L'activation des neurones de sortie s'interprête comme des probabilités dans l'intervalle [0,1].
        \end{itemize}
        }
    \end{minipage}

  \end{tcolorbox}
  Le neurone de plus grande probabilité (activation) donne la réponse du réseau (label associé au neurone).  

\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}

  \begin{tcolorbox}[title=Codage {\em One-hot} des labels]  

    But : mettre les label des images au format de la sortie du réseau

    {\small
        \begin{itemize}
        \item Labels des images  : \textbf{nombres entiers} de 0 à 9.
        \item Sortie du réseau : \textbf{vecteur de 10 \texttt{float}} dans l'intervalle [0,1] calculés par les fonctions {\em softmax} des neurones de sortie.
        \item Principe du codage {\em one-hot} d'un ensemble ordonné d'éléments : \\[1mm]
          \hspace*{20mm}\includegraphics[width=.45\textwidth]{images/oneHotCoded-2.png}\\[-2mm]
                          \hspace*{25mm}{\tiny (image : \href{https://www.educative.io/blog/one-hot-encoding}{www.educative.io/blog/one-hot-encoding})}
        \end{itemize}
      }
  \end{tcolorbox}  

  \begin{minipage}{.25\textwidth}
      \vspace*{-4.5mm}\hspace*{4mm}\includegraphics[width=.8\textwidth]{images/oneHotCoded.png}
    \end{minipage}
    \begin{minipage}{.65\textwidth}
      Le codage {\em one-hot} des labels 0 à 9 donne un vecteur de même dimension que celui calculé par le réseau de neurones.
    \end{minipage}
    
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}

  \begin{tcolorbox}[title=Fonction d'erreur : {\em Cross entropy error}]  

    \begin{itemize}
    \item Une image traitée par le réseau $\leadsto$ vecteur $Y$ de 10 \texttt{float} à comparer au codage {\em hot-one} $Y'$ du label de l'image.
    \item On utilise la fonction d'erreur (ou de perte) {\em cross entropy} adaptée au codage {\em one-hot} : $e(Y,Y')=-\sum_i Y^{'}_i\log(Y_i)$\\
      \includegraphics[width=.9\textwidth]{images/CrossEntropy-2.png}\\[-2mm]
      \hspace*{1.5cm}{\tiny (image : \href{https://www.youtube.com/watch?v=BtAVBeLuigI}{vidéo "Deep Learning TensorFlow" de Martin Gorner})}
    \end{itemize}

  \end{tcolorbox}
  
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}

  \begin{tcolorbox}[title=Optimisation et {\em Back Propagation}]  

    \begin{itemize}
    \item Pendant la phase d'apprentissage un algorithme d'optimisation calcule le gradient de la fonction de perte
      par rapport aux poids du réseau.
    \item Exemples d'algorithme d'optimisation utilisés : {\em Gradient Descent (GD)}, {\em Stochastic Gradient Descent (SGD)}, {\em Adam}...
    \item L'algorithme de {\em Back Propagation} \textbf{modifie} les poids du réseau en utilisant le calcul du gradient de la fonction de perte couche par couche,
      en itérant en arrière à partir de la dernière couche. 
    \end{itemize}
  \end{tcolorbox}
  \hspace*{30mm}\href{https://www.3blue1brown.com/lessons/backpropagation}{\includegraphics[width=.35\textwidth]{images/video-BackPropagation.png}}
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}

  {\footnotesize On peut visualiser les itérations des algorithmes de descente de gradient dans le cas très simplifié d'une fonction de perte à seulement 2 variables :}\\[2mm]
  \hspace*{8mm}\href{https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif}{\includegraphics[width=.8\textwidth]{images/adam_plot3D_animated.png}}\\[-2mm]
  %\hspace*{8mm}\includegraphics[width=.8\textwidth]{images/adam_plot3D_animated.png}\\[-2mm]
  %\animategraphics[width=.8\textwidth,controls]{10}{./images/Adam-plot3D/movie9/img_}{0}{114}
  \hspace*{25mm}{\tiny (source : \href{https://github.com/Jaewan-Yun/optimizer-visualization}{github.com/Jaewan-Yun/optimizer-visualization})}
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{1 - Réseau de neurones dense}

  \begin{tcolorbox}[title=Mise en oeuvre dans l'APP2]
    \begin{itemize}
    \item Le {\em notebook jupyter} \DarkBlue{\texttt{PB1\_MNIST\_dense.ipynb}} détaille la conception et la mise en oeuvre d'un réseau dense pour la
      reconnaissance des images de chiffres MNIST.
    \item Les modules Python utilisés pour créer des réseaux denses et les entraîner sont \Chocolate{\texttt{tensorflow}} et \Chocolate{\texttt{keras}}.
    \item Les scores obtenus avec des réseaux denses peuvent atteindre 98\% de réussite avec les images de test.
    \end{itemize}
  \end{tcolorbox}

\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{2 - Réseau de neurones convolutif}

  {\small Pour dépasser le score de 98\% de réussite, il faut passer à des réseaux plus complexes
    spécialisés dans le traitement des images : les \textbf{réseaux de neurones convolutifs} (RNC), ou {\em Convolutional Neural Netowrk (CNN)}.}
  \smallskip

  \begin{tcolorbox}[title=Mise en oeuvre dans l'APP2]
    Le {\em notebook} \DarkBlue{\texttt{PB2\_MNIST\_convol.ipynb}} détaille la conception et la mise en oeuvre d'un RNC spécialisé pour la
      reconnaissance des images, inspiré du réseau \textbf{LeNet5} (un des premiers RNC proposé par Yann LeCun {\em et al.} dans les années 90) :\\
      \hspace*{5mm}\includegraphics[width=.9\textwidth]{images/LeNet5-2.png}\\
      {\tiny \href{https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition}{Yann Lecun {\em et al.}, 1998, "Gradient-based learning applied to document recognition", Proceedings of the IEEE. 86 (11)}}
\end{tcolorbox}
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{Vidéographie}

  \hspace*{-3mm}
  \href{https://youtu.be/trWrEWfhTVg}{\includegraphics[width=.5\textwidth]{images/video-LeDeepLearning.png}}
  \hfill%
  \href{https://www.youtube.com/watch?v=aircAruvnKk}{\includegraphics[width=.5\textwidth]{images/video-NeuralNetworks.png}}\\[-2mm]

  {\tiny\hspace*{-2mm}\href{run:./videos/Le deep learning - YouTube.webm}{1/ Le deep learning - YouTube.webm}%
    \hfill%
    \href{run:./videos/But what is a neural network Chapter 1, Deep learning - You.webm}{2/ But what is a neural network... .webm}}\\[2mm]
            
  \hspace*{-3mm}
  \href{https://www.youtube.com/watch?v=IHZwWFHWa-w}{\includegraphics[width=.5\textwidth]{images/video-HowMachineLearn.png}}
  \hfill
  \href{https://www.3blue1brown.com/lessons/backpropagation}{\includegraphics[width=.5\textwidth]{images/video-BackPropagation.png}}\\[-2mm]
  
  {\tiny\hspace*{-3mm}\href{run:./videos/Gradient descent how neural networks learn, Chapter 2.webm}{3/ Gradient descent how neural networks learn... .webm}%
    \hfill%
    \href{run:./videos/What is backpropagation really doing Chapter 3.webm}{4/ What is backpropagation really doing... .webm}}\\[2mm]
  
\end{frame}
%===============================================================================

%===============================================================================
\begin{frame}{Biliographie}
  
  \noindent\fontsize{8}{8}\selectfont{

    \hspace*{-4mm}\hypertarget{refRusselNorvig}{[1] }%
    {\em Intelligence Artificielle}, 3e édition, PEARSON Education, 2010, ISBN : 2-7440--7455--4,
    \href{http://aima.cs.berkeley.edu/}{aima.cs.berkeley.edu}\\[5mm]


    \hspace*{-4mm}\hypertarget{refStrongWeak-AI}{[2] }%
    {\em What is artificial intelligence (AI), and what is the difference between general AI and narrow AI?}, Kris Hammond, 2015\\
    \href{https://www.computerworld.com/article/2906336/what-is-artificial-intelligence.html}
         {www.computerworld.com/article/2906336/what-is-artificial-intelligence.html}\\[5mm]

    \hspace*{-4mm}\hypertarget{StanfordEncyc}{[3] }%
    {\em Stanford Encyclopedia of Philosophy},
    \href{https://plato.stanford.edu/entries/artificial-intelligence}
         {plato.stanford.edu/entries/artificial-intelligence}\\[5mm]

    \hspace*{-4mm}\hypertarget{DeepLeraning}{[4] }%
    {\em  Deep Learning.}, Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), MIT Pres, ISBN 9780262035613
  }
  
\end{frame}
%===============================================================================


\end{document}



[ML-01] – JLC  
« Machine Learning for Humans » 
https://medium.com/machine-learning-for-humans

Point d'entrée du site « Machine Learning for Humans », à partir duquel on peut aller sur "A Beginner's Guide to AI/ML" et d'autres documents très intéressants.


[ML-02] – JLC  
« A Beginner’s Guide to AI/ML »
https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12

Cours complet : supervised learning, unsupervised learning, Neural networks & deep learning, reinforcement learning....Version PDF du cours également disponible. sur : https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0

[ML-03] – VG
« ML Basics : Unsupervised, supervised and reinforcement learning »
Article court mais utile pour bien saisir la différence entre supervised, unsupervised et reinforcement learning.
https://medium.com/@machadogj/ml-basics-supervised-unsupervised-and-reinforcement-learning-b18108487c5a

DRL  (Deep Reinforcement Learning)
[DRL-01] – JLC 
« Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG) »
https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287

[DRL-02] – VG
https://deepmind.com/blog/deep-reinforcement-learning/
Récapitulatif de ce qu’on sait faire aujourd’hui en DRL fait par DeepMind (un des leaders mondiaux en DRL, concepteurs d’AlphaGo notamment) 

[DRL-03] – JLC
« Demystifying Deeep Reinforcement Learning », Tambet Matiisen
https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
Article très clair et très pédagogique sur ML, RL et DQN.


4 types of deep learning neural networks

There are four types of neural networks:

    Convolutional neural networks: Mostly used for analyzing and classifying images, the architecture of a convolutional neural network is analogous to the organization and connectivity patterns of the visual cortex in the human brain.
    Recurrent neural networks: Used in the field of NLP and sequence prediction problems, the algorithm captures information about previous computations to influence subsequent output.
    Recursive neural networks: These neural networks process input hierarchically in a tree fashion. For example, a recurrent neural network algorithm would analyze a sentence by parsing it into smaller chunks of text or individual words.
    Generative adversarial networks: These networks can generate photographs that appear to be authentic to the human eye by taking photographic data and shaping the elements into realistic-looking objects (e.g., people, animals, locations).


    \subsection{main steps}

%===============================================================================
\begin{frame}{AI recent spots}

  \begin{itemize}
  \item May 11, 1997, the IBM computer \href{https://www-03.ibm.com/ibm/history/ibm100/us/en/icons/deepblue/}{Deep Blue} beat the world chess champion.
  \item 2015 Google trained a conversational agent that could interact with humans, discuss morality, express opinion....
  \item 2015 Google \href{https://deepmind.com}{deepmind} developped an agent that surpassed human performances at 49 Atari games
  \end{itemize}

\end{frame}
%===============================================================================

\footnote{\tiny \hyperlink{DeepLeraning}{[2]} {\em Deep Learning.}, Goodfellow {\em et al.}, Chapitre {\em "6.5 Back-Propagation and Other Differentiation Algorithms"}}
